# moe-gpt2
Simple architecture for based on Transformers architecture and GPT2 Tokenizer

Training, testing and validation is done Wikipedia datasets. 